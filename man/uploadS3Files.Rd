% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ImportResults.R
\name{uploadS3Files}
\alias{uploadS3Files}
\title{Insert a set of files into DB from s3 bucket objects}
\usage{
uploadS3Files(manifestDf, connectionDetails, targetSchema, cdmInfo)
}
\value{
list of load tables (which are based on spawned proc ids) - to be merged by calling proc
}
\description{
Creates a load table for insert of all objects.

Checks insert of chunks by validating last entry of data file into table - if entry is found then data is not inserted
S3 Object is deleted when it is inserted or if it clashes with a primary key

This caused the insert to succeed but the error code causes a crash in R

Note, this function also works around a bug in readr/vroom whereby a temproary file is created when opening the
s3 bucket. This file is not deleted until the thread closes

This file is the size of the csv.gz so quickly fills up the system's temp space which will lead to a catestrophic
failure on windows.

Consequently, we set VROOM_TEMP_PATH to a customizable dir. This process is also lightning
fast if you use the ram as a disk store for the tempdir as DatabaseConnector also writes a tempfile.
On linux this is trivial but this can be achieved on windows server with some configuration:
}
\seealso{
http://woshub.com/create-ram-disk-windows-server/


Note the use of load per thread tables is because pgcopy is not threadsafe with inserts (i.e. there is no locking)
}
